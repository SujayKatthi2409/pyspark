
  Agenda (PySpark with Databricks)
  ---------------------------------------------
   Spark basic components and architecture				
   Spark APIs									
   Databricks platform basics						
   Spark Core - RDD API							
	RDD Transformations and Actions
	Spark Shared Variables
   Spark SQL									
	DataFrame Transformations 
	DataFrame integration with MySQL
	DataFrame integration with Hive
	Running SQL on Local & Global Temporary Views
   SQL optimization and performance tuning				
   Spark Structured Streaming						
   Delta Lake									
	Delta Lake architecture
	Working with Delta tables - INSERT, UPDATE, DELETE & MERGE
	Time travel (Restore to a point-in-time snapshots)
	Vacuuming of Delta tables
	Compaction of Delta tables

  Materials
  ---------
	=> PDF Presentations
	=> Code Modules 
	=> Databricks Notebooks 
	=> Class Notes
        => Github: https://github.com/ykanakaraju/pyspark

  Spark
  -----

    => Spark is in-memory distributed computing framework for big data analytics.

	in-memory: ability to persist intermediate results in RAM and subsequent operations
		   can directly work on these persisted intermediate results. 

    => Spark is written in Scala programming language

    => Spark is a polyglot
	-> Scala, Java, Python, R (and SQL)

    => Spark applications can run on:
	-> local, Spark standalone, YARN, Mesos, Kubernetes

    => Spark is unified analytics framework.	

 
  Spark Unified Framework
  -----------------------
	Spark provides a consistent set of APIs for performing different analytics workloads
        using the same execution engine and some well defined data abstractions and operations.

   	Batch Analytics of unstructured data	-> Spark Core API (low-level api)
	Batch Analytics of structured data 	-> Spark SQL
	Streaming Analytics (real time)		-> Spark Streaming (DStreams API), Structured Streaming
	Predictive analytics (ML)		-> Spark MLLib  (mllib & ml)
	Graph parallel computations  		-> Spark GraphX


  Getting started with Spark on Databricks
  ----------------------------------------   
        ** Databricks Community Edition (free edition)
 		
	Signup: https://www.databricks.com/try-databricks
		Screen 1: Fill up the details with valid email address
		Screen 2: Click on "Get started with Community Edition" link (Not the 'Continue' button)

	Login: https://community.cloud.databricks.com/login.html

	Downloading a file from Databricks
	----------------------------------
		/FileStore/<FILEPATH>
		https://community.cloud.databricks.com/files/<FILEPATH>?o=1072576993312365

		Example:
		dbfs:/FileStore/output/wc/part-00000
		https://community.cloud.databricks.com/files/output/wc/part-00000?o=1072576993312365

		dbfs:/FileStore/output/wordcount1/part-00000
		https://community.cloud.databricks.com/files/output/wordcount1/part-00000?o=1072576993312365


 	Enabling DBFS File browser
	--------------------------
	<your account (top-right)> -> Admin settings -> Advanced -> Other -> DBFS File Browser (enable it)
  




    

